{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass CNN classification on \"MNIST dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2019 The TensorHub Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorhub.models.image.classifiers import VGG16 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_n_normalize_image(small_img, b=32, h=32):\n",
    "    img = Image.fromarray(small_img, \"L\") # reading a gray scale image from array\n",
    "    img = img.convert(mode='RGB') # added 3 channels to grapy scale image\n",
    "    img = img.resize((b, h), Image.ANTIALIAS) # resizing the image\n",
    "    return np.array(img)/255.0 # normalizing the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnist images are 28*28 images\n",
      "\n",
      "After resizing\n",
      "train data shape: (60000, 32, 32, 3)\n",
      "test data shape: (10000, 32, 32, 3) \n",
      "\n",
      "train labels shape: (60000, 10)\n",
      "test labels shape: (10000, 10) \n",
      "\n",
      "MNIST data:\n",
      "\n",
      "sample image in training data\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample training lables\n",
      "[5 0 4 1 9 2 1 3 1 4]\n",
      "\n",
      "training lables after one hot encoding\n",
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "\n",
    "# normalizing & resizing training and test images \n",
    "b, h = 32, 32\n",
    "train_images = train_images.reshape((60000, 28 , 28)) # original mnist image are of 28*28 dimensions\n",
    "train_images = np.array([resize_n_normalize_image(image_, b, h) for image_ in train_images])\n",
    "\n",
    "test_images = test_images.reshape((10000, 28 , 28)) # original mnist image are of 28*28 dimensions\n",
    "test_images = np.array([resize_n_normalize_image(image_, b, h) for image_ in test_images])\n",
    "\n",
    "# Preparing the labels\n",
    "temp_train_labels = train_labels[:10]\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels = to_categorical(test_labels)\n",
    "\n",
    "print (\"mnist images are 28*28 images\\n\")\n",
    "print (\"After resizing\")\n",
    "print('train data shape:', train_images.shape)\n",
    "print('test data shape:', test_images.shape, \"\\n\")\n",
    "\n",
    "print('train labels shape:', train_labels.shape)\n",
    "print('test labels shape:', test_labels.shape, \"\\n\")\n",
    "\n",
    "\n",
    "print(\"MNIST data:\")\n",
    "print (\"\\nsample image in training data\")\n",
    "digit = train_images[0]\n",
    "plt.imshow(np.reshape(digit, (b,h,3)), cmap=plt.cm.binary)\n",
    "plt.show()\n",
    "\n",
    "print (\"sample training lables\")\n",
    "print (temp_train_labels)\n",
    "print (\"\\ntraining lables after one hot encoding\")\n",
    "print(train_labels[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Model architectuire "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         [(None, 32, 32, 3)]       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 32, 32, 64)        1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 16, 16, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 8, 8, 256)         295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 4, 4, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 1, 1, 512)         0         \n",
      "_________________________________________________________________\n",
      "sequential_8 (Sequential)    (None, 10)                1055242   \n",
      "=================================================================\n",
      "Total params: 15,769,930\n",
      "Trainable params: 15,769,930\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "n_classes = 10 # number of classes\n",
    "model = VGG16(n_classes=n_classes, img_width=b, img_height=h)\n",
    "\n",
    "\n",
    "# model compile\n",
    "model.compile(optimizer='rmsprop',\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6000 samples, validate on 1000 samples\n",
      "Epoch 1/5\n",
      "6000/6000 [==============================] - 94s 16ms/sample - loss: 14.2525 - accuracy: 0.1075 - val_loss: 14.5224 - val_accuracy: 0.0990\n",
      "Epoch 2/5\n",
      "6000/6000 [==============================] - 94s 16ms/sample - loss: 14.3693 - accuracy: 0.1085 - val_loss: 14.5224 - val_accuracy: 0.0990\n",
      "Epoch 3/5\n",
      "6000/6000 [==============================] - 99s 17ms/sample - loss: 14.3693 - accuracy: 0.1085 - val_loss: 14.5224 - val_accuracy: 0.0990\n",
      "Epoch 4/5\n",
      "6000/6000 [==============================] - 100s 17ms/sample - loss: 14.3639 - accuracy: 0.1088 - val_loss: 14.5224 - val_accuracy: 0.0990\n",
      "Epoch 5/5\n",
      "6000/6000 [==============================] - 98s 16ms/sample - loss: 14.3773 - accuracy: 0.1080 - val_loss: 14.5224 - val_accuracy: 0.0990\n"
     ]
    }
   ],
   "source": [
    "train_hist = model.fit(train_images[:6000], train_labels[:6000], \n",
    "                       epochs=5, batch_size=64, \n",
    "                       validation_data=(test_images[:1000], test_labels[:1000]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
